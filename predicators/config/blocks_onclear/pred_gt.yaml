final_op: [1, 0, 0, 0, 0]
neupi_non_effect_predicates: []
config:
  - name: "neural_u_p1"
    types: ["robot"]
    ent_idx: [0]
    gt: [[0, 2, 1, 1, 2]]
    architecture: 
      type: "MLP"
      layer_size: 32
      initializer: "xavier"
    optimizer:
      type: "AdamW"
      kwargs:
        lr: 0.001
    lr_scheduler:
      type: "StepLR"
      kwargs:
        step_size: 70
        gamma: 0.1
    batch_vect_num: 12
    batch_size: 512
    epochs: 100
    gumbel_temp: 0.66
    val_freq: 10
    num_iter: 5
    matrix_vec_try: 100
    search_tree_max_level: 3
    guidance_thresh: 0.1 # this predicate is harder to learn for nn
    loss_thresh: 0.1
    skip_train: False
  
  - name: "neural_u_p2"
    types: ["block"]
    ent_idx: [0]
    gt: [[0, 2, 1, 0, 0]]
    architecture:
      type: "MLP"
      layer_size: 32
      initializer: "xavier"
    optimizer:
      type: "AdamW"
      kwargs:
        lr: 0.001
    lr_scheduler:
      type: "StepLR"
      kwargs:
        step_size: 70
        gamma: 0.1
    batch_vect_num: 12
    ucb_kappa: 0.2
    batch_size: 512
    epochs: 100
    gumbel_temp: 0.66
    val_freq: 10
    num_iter: 5
    matrix_vec_try: 100
    search_tree_max_level: 1
    guidance_thresh: 0.1 # this predicate is harder to learn for nn
    loss_thresh: 0.1
    skip_train: False

  - name: "neural_b_p3"
    types: ["block", "block"]
    ent_idx: [0, 1]
    gt: [[0, 0, 0, 1, 2]]
    architecture:
      type: "MLP"
      layer_size: 128
      initializer: "xavier"
    optimizer:
      type: "AdamW"
      kwargs:
        lr: 0.001
    lr_scheduler:
      type: "StepLR"
      kwargs:
        step_size: 100
        gamma: 0.1
    batch_vect_num: 4
    ucb_kappa: 0.3
    batch_size: 512
    epochs: 200
    gumbel_temp: 0.66
    val_freq: 10
    num_iter: 70
    matrix_vec_try: 100
    search_tree_max_level: 1
    guidance_thresh: 0.1 # this predicate is harder to learn for nn
    loss_thresh: 0.1
    skip_train: False

  - name: "neural_b_p4"
    types: ["robot", "block"]
    ent_idx: [0, 0]
    gt: [[0, 1, 2, 2, 1]]
    architecture:
      type: "MLP"
      layer_size: 128
      initializer: "xavier"
    optimizer:
      type: "AdamW"
      kwargs:
        lr: 0.001
    lr_scheduler:
      type: "StepLR"
      kwargs:
        step_size: 70
        gamma: 0.1
    batch_vect_num: 4
    ucb_kappa: 0.3
    batch_size: 512
    epochs: 20
    gumbel_temp: 0.66
    val_freq: 10
    num_iter: 70
    matrix_vec_try: 100
    search_tree_max_level: 1
    guidance_thresh: 0.1
    loss_thresh: 0.1
    skip_train: False